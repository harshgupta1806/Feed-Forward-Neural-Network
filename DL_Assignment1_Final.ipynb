{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "M8_d0DATWE35"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From c:\\Users\\DELL\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Importing necessary libraries\n",
        "from keras.datasets import fashion_mnist  # Importing Fashion MNIST dataset from Keras\n",
        "import numpy as np  # Importing NumPy library for numerical operations\n",
        "import matplotlib.pyplot as plt  # Importing Matplotlib library for plotting"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qsgn6H8KWE4B"
      },
      "source": [
        "# **Question-1**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sw4DeVb7WE4F",
        "outputId": "f80f7e02-4779-48e5-ba74-0d3ad8072d0f"
      },
      "outputs": [],
      "source": [
        "#loading data set\n",
        "(x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "CXn3fmt8WE4G"
      },
      "outputs": [],
      "source": [
        "# Checking Dataset shape\n",
        "# print(x_train.shape, y_train.shape, x_test.shape, y_test.shape)\n",
        "# type(x_train)\n",
        "# x_train = np.reshape(x_train, (x_train.shape[0], x_train.shape[1] * x_train.shape[1]))\n",
        "# x_train.shape\n",
        "\n",
        "def input_matrix(image):\n",
        "    \"\"\"\n",
        "    Reshape and normalize input images.\n",
        "    \n",
        "    Parameters:\n",
        "    - image: Input image\n",
        "    \n",
        "    Returns:\n",
        "    - Reshaped and normalized input matrix\n",
        "    \"\"\"\n",
        "    return image.reshape(image.shape[0], -1) / 255.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "8RPjO_d6WE4H"
      },
      "outputs": [],
      "source": [
        "# class labels for fashion mnist dataset\n",
        "label = {\n",
        "     0 :  \"T-shirt/top\",\n",
        "     1 :  \"Trouser\",\n",
        "     2 :  \"Pullover\",\n",
        "     3 :  \"Dress\",\n",
        "     4 :  \"Coat\",\n",
        "     5 :  \"Sandal\",\n",
        "     6 :  \"Shirt\",\n",
        "     7 :  \"Sneaker\",\n",
        "     8 :  \"Bag\",\n",
        "     9 :  \"Ankle boot\"\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 400
        },
        "id": "w7WgHTlVWE4J",
        "outputId": "8db96df2-602a-4cca-dda7-275fe028590d"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 600x600 with 10 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Creating subplots to display one image of each class\n",
        "fig, axes = plt.subplots(2, 5, figsize=(6, 6))\n",
        "\n",
        "# Iterating through each class\n",
        "for i in range(len(label)):\n",
        "    row = i // 5  # Calculating row index for subplot\n",
        "    col = i % 5   # Calculating column index for subplot\n",
        "    \n",
        "    # Finding the index of the first image in the training set with label i\n",
        "    idx = np.argmax(y_train == i)\n",
        "    \n",
        "    # Displaying the image corresponding to the label i\n",
        "    axes[row, col].imshow(x_train[idx], cmap='gray')  # Displaying grayscale image\n",
        "    axes[row, col].set_title(label[i])  # Setting title with class label\n",
        "    axes[row, col].axis('off')  # Turning off axis\n",
        "\n",
        "# Adjusting the layout to prevent overlapping\n",
        "plt.tight_layout()\n",
        "\n",
        "# Displaying the plot\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wAkLUv9AWE4L"
      },
      "source": [
        "# **Activation Functions**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "M2C0q0MdWE4M"
      },
      "outputs": [],
      "source": [
        "\"\"\" Activation Class :- Contains various diffrent activations functions \"\"\"\n",
        "class Activation_Functions:\n",
        "    def __init__(self) -> None:\n",
        "        pass\n",
        "\n",
        "\n",
        "    def sigmoid(self, x):\n",
        "        # Compute sigmoid element-wise for each element of the matrix\n",
        "        sigmoid_x = np.zeros_like(x)  # Initialize output matrix with zeros\n",
        "        \n",
        "        # Apply the sigmoid function element-wise using vectorized operations\n",
        "        positive_mask = x >= 0\n",
        "        sigmoid_x[positive_mask] = 1.0 / (1.0 + np.exp(-x[positive_mask]))\n",
        "        sigmoid_x[~positive_mask] = np.exp(x[~positive_mask]) / (1.0 + np.exp(x[~positive_mask]))\n",
        "        \n",
        "        return sigmoid_x\n",
        "\n",
        "\n",
        "    def ReLU(self, x):\n",
        "        return np.maximum(0, x)\n",
        "\n",
        "    def tanh(self, x):\n",
        "        return np.tanh(x)\n",
        "\n",
        "    \n",
        "\n",
        "    def softmax(self, x):\n",
        "        # Subtract the maximum value along the axis to prevent overflow\n",
        "        max_x = np.max(x, axis=1, keepdims=True)\n",
        "        exp_x = np.exp(x - max_x)\n",
        "        \n",
        "        # Compute softmax probabilities\n",
        "        softmax_x = exp_x / np.sum(exp_x, axis=1, keepdims=True)\n",
        "        \n",
        "        return softmax_x\n",
        "\n",
        "\n",
        "    def activation(self, x, fun):\n",
        "        if fun == \"tanh\":\n",
        "            return self.tanh(x)\n",
        "        elif fun == \"sigmoid\":\n",
        "            return self.sigmoid(x)\n",
        "        elif fun == \"ReLU\":\n",
        "            return self.ReLU(x)\n",
        "        elif fun == \"softmax\":\n",
        "            return self.softmax(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# **Derivatives**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "156_-ifDWE4O"
      },
      "outputs": [],
      "source": [
        "class Derivatives:\n",
        "    def __init__(self) -> None:\n",
        "        \"\"\" \n",
        "            Constructor method for Derivatives class.\n",
        "            Initialization of object of Activation_Functions class\n",
        "        \"\"\"\n",
        "        self.fun = Activation_Functions()  # Creating an instance of Activation_Functions class\n",
        "\n",
        "    \n",
        "    def sigmoid_derivative(self, x):\n",
        "        \"\"\"\n",
        "        Computes the derivative of the sigmoid activation function.\n",
        "        \n",
        "        Parameters:\n",
        "        - x: Input value\n",
        "        \n",
        "        Returns:\n",
        "        - Derivative of the sigmoid activation function\n",
        "        \"\"\"\n",
        "        g = self.fun.sigmoid(x)  # Computing sigmoid activation\n",
        "        return g * (1 - g)  # Computing and returning derivative\n",
        "\n",
        "    \n",
        "    def tanh_derivative(self, x):\n",
        "        \"\"\"\n",
        "        Computes the derivative of the hyperbolic tangent (tanh) activation function.\n",
        "        \n",
        "        Parameters:\n",
        "        - x: Input value\n",
        "        \n",
        "        Returns:\n",
        "        - Derivative of the tanh activation function\n",
        "        \"\"\"\n",
        "        g = self.fun.tanh(x)  # Computing tanh activation\n",
        "        return 1 + g * g  # Computing and returning derivative\n",
        "\n",
        "    \n",
        "    def ReLU_derivative(self, x):\n",
        "        \"\"\"\n",
        "        Computes the derivative of the Rectified Linear Unit (ReLU) activation function.\n",
        "        \n",
        "        Parameters:\n",
        "        - x: Input value\n",
        "        \n",
        "        Returns:\n",
        "        - Derivative of the ReLU activation function\n",
        "        \"\"\"\n",
        "        g = self.fun.ReLU(x)  # Computing ReLU activation\n",
        "        return np.where(g > 0, 1, 0)  # Computing and returning derivative\n",
        "\n",
        "\n",
        "    def derivatives(self, x, activation_function):\n",
        "        \"\"\"\n",
        "        Computes the derivative of a specified activation function.\n",
        "        \n",
        "        Parameters:\n",
        "        - x: Input value\n",
        "        - activation_function: Name of the activation function\n",
        "        \n",
        "        Returns:\n",
        "        - Derivative of the specified activation function\n",
        "        \"\"\"\n",
        "        if activation_function == \"sigmoid\":\n",
        "            return self.sigmoid_derivative(x)  # Computing derivative for sigmoid activation\n",
        "        elif activation_function == \"tanh\":\n",
        "            return self.tanh_derivative(x)  # Computing derivative for tanh activation\n",
        "        elif activation_function == \"ReLU\":\n",
        "            return self.ReLU_derivative(x)  # Computing derivative for ReLU activation\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# **Loss Functions**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "v6eUMIdbWE4P"
      },
      "outputs": [],
      "source": [
        "class Loss_Function:\n",
        "    def __init__(self) -> None:\n",
        "        \"\"\"\n",
        "        Constructor method for Loss_Function class.\n",
        "        Initializes the default loss function to cross-entropy.\n",
        "        \"\"\"\n",
        "        self.default_loss_function = \"cross_entropy\"\n",
        "\n",
        "\n",
        "    def compute_loss(self, y_true, y_hat, loss_function):\n",
        "        \"\"\"\n",
        "        Computes the loss based on the given true labels and predicted probabilities.\n",
        "        \n",
        "        Parameters:\n",
        "        - y_true: True labels (one-hot encoded) \n",
        "        - y_hat: Predicted probabilities\n",
        "        - loss_function: Name of the loss function to be used\n",
        "        \n",
        "        Returns:\n",
        "        - Loss value\n",
        "        \"\"\"\n",
        "\n",
        "        \n",
        "        if loss_function == None: \n",
        "            # If no loss function is specified, use the default loss function\n",
        "            loss_function = self.default_loss_function\n",
        "\n",
        "        if loss_function == \"cross_entropy\":\n",
        "            # Set a small value epsilon to avoid numerical instability\n",
        "            epsilon = 1e-15\n",
        "            # Clip the predicted values to avoid log(0) and log(1) scenarios\n",
        "            y_hat = np.clip(y_hat, epsilon, 1. - epsilon)\n",
        "            # Compute the cross-entropy loss for each sample\n",
        "            loss = -np.sum(y_true * np.log(y_hat), axis=1)\n",
        "            # Compute the mean loss across all samples\n",
        "            loss = np.mean(loss)\n",
        "            # Return the computed loss\n",
        "            return loss\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# **Question - 2**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "65CnAaCRWE4Q"
      },
      "outputs": [],
      "source": [
        "class Network:\n",
        "    def __init__(self, PARAM):\n",
        "        \"\"\"\n",
        "        Constructor method for Network class.\n",
        "        \n",
        "        Parameters:\n",
        "        - PARAM: Dictionary containing network parameters (input size, hidden layers, output size)\n",
        "        \"\"\"\n",
        "        self.weight = {}  # Dictionary to store weights of each layer\n",
        "        self.bias = {}    # Dictionary to store biases of each layer\n",
        "        self.a = {}       # Dictionary to store activations of each layer\n",
        "        self.h = {}       # Dictionary to store outputs of each layer\n",
        "        self.fun = Activation_Functions()  # Instance of Activation_Functions class\n",
        "        self.size_list = [PARAM[\"input_size\"]] + PARAM[\"hidden_layers\"] + [PARAM[\"output_size\"]]  # List containing sizes of all layers\n",
        "        self.y_predictions = []  # List to store predicted probabilities for each input sample\n",
        "\n",
        "\n",
        "    def initialize_parameters(self):\n",
        "        \"\"\"\n",
        "        Method to initialize weights and biases of the network.\n",
        "        \"\"\"\n",
        "        for layer in range(1, len(self.size_list)):\n",
        "            self.weight[layer] = np.random.randn(self.size_list[layer-1], self.size_list[layer])  # Initializing weights with random values\n",
        "            self.bias[layer] = np.random.randn(1, self.size_list[layer])  # Initializing biases with random values\n",
        "\n",
        "\n",
        "    def forward_pass(self, x, activation_function):\n",
        "        \"\"\"\n",
        "        Method to perform forward pass through the network.\n",
        "        \n",
        "        Parameters:\n",
        "        - x: Input data\n",
        "        - activation_function: Name of the activation function to be used\n",
        "        \n",
        "        Returns:\n",
        "        - Output of the final layer (after applying softmax activation)\n",
        "        \"\"\"\n",
        "        total_layer = len(self.size_list)\n",
        "        self.h[0] = x  # Input layer\n",
        "\n",
        "        for layer in range(1, total_layer-1):\n",
        "            self.a[layer] = np.dot(self.h[layer-1], self.weight[layer]) + self.bias[layer]  # Computing preactivation\n",
        "            self.h[layer] = self.fun.activation(self.a[layer], fun=activation_function)  # Applying activation function\n",
        "    \n",
        "        self.a[total_layer-1] = np.dot(self.h[layer], self.weight[total_layer-1]) + self.bias[total_layer-1]  # Computing weighted sum for final layer\n",
        "        self.h[total_layer-1] = self.fun.activation(self.a[total_layer-1], fun=\"softmax\")  # Applying softmax activation\n",
        "        \n",
        "        return self.h[total_layer-1]  # Returning output of final layer\n",
        "\n",
        "\n",
        "    def predict_probability(self, dataset, activation):\n",
        "        \"\"\"\n",
        "        Method to predict probabilities for each input sample in the dataset.\n",
        "        \n",
        "        Parameters:\n",
        "        - dataset: Input dataset\n",
        "        - activation: Name of the activation function to be used\n",
        "        \n",
        "        Returns:\n",
        "        - List containing predicted probabilities for each input sample\n",
        "        \"\"\"\n",
        "        self.initialize_parameters()  # Initializing network parameters\n",
        "\n",
        "        for image in dataset:\n",
        "            x = image.reshape(1, -1) / 255.0  # Reshaping and normalizing input data\n",
        "            y_hat = self.forward_pass(x, activation)  # Performing forward pass\n",
        "            self.y_predictions.append(y_hat)  # Storing predicted probabilities\n",
        "            \n",
        "        return self.y_predictions  # Returning list of predicted probabilities\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "U7RFdFL0WE4R"
      },
      "outputs": [],
      "source": [
        "PARAM = {\n",
        "    \"input_size\" : 784,            # Size of the input layer (number of input features)\n",
        "    \"hidden_layers\" : [5, 6, 7],   # Sizes of hidden layers in the neural network\n",
        "    \"output_size\" : 10             # Size of the output layer (number of classes)\n",
        "}\n",
        "\n",
        "n1 = Network(PARAM)  # Creating an instance of the Network class with the given parameters\n",
        "y_pred = n1.predict_probability(x_train, \"sigmoid\")  # Predicting probabilities for each sample in the training dataset using sigmoid activation function\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "49bpLEBNWE4S",
        "outputId": "20d3ab6a-9f3c-42d3-eace-4137f48841aa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[0.00348156 0.00380564 0.00512167 0.01079694 0.00228669 0.00014978\n",
            "  0.00011048 0.96998277 0.0039166  0.00034787]]\n"
          ]
        }
      ],
      "source": [
        "np.set_printoptions(suppress=True)  # Suppressing scientific notation in printed arrays\n",
        "print(y_pred[0])  # Printing the predicted probabilities for the first sample"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# **Question - 3**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r2q1H-gPWE4S"
      },
      "source": [
        "# **Optimizers**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "class Optimizer:\n",
        "    def __init__(self, neural_network, PARAM) -> None:\n",
        "        \"\"\"\n",
        "        Constructor method for Optimizer class.\n",
        "        \n",
        "        Parameters:\n",
        "        - neural_network: Instance of the neural network class\n",
        "        - PARAM: Dictionary containing optimization parameters (eta, weight_decay, optimizer, beta)\n",
        "        \"\"\"\n",
        "        self.neural_network = neural_network  # Neural network object\n",
        "        self.eta = PARAM[\"eta\"]  # Learning rate\n",
        "        self.weight_decay = PARAM[\"weight_decay\"]  # Weight decay factor\n",
        "        self.optimizer = PARAM[\"optimizer\"]  # Optimization algorithm (sgd, mgd, nag)\n",
        "        self.beta = PARAM[\"beta\"]  # Momentum factor for momentum-based optimization\n",
        "        self.epsilon = PARAM[\"epsilon\"]\n",
        "        self.beta2 = PARAM[\"beta2\"]\n",
        "\n",
        "    def stochastic_gradient_decent(self):\n",
        "        \"\"\"\n",
        "        Method to perform stochastic gradient descent optimization.\n",
        "        \"\"\"\n",
        "        weight_decay = self.weight_decay\n",
        "        eta = self.eta\n",
        "\n",
        "        for layer in range(len(self.neural_network.size_list)-1, 0, -1):\n",
        "            decay_wt = weight_decay * self.neural_network.weight[layer]  # Applying weight decay\n",
        "            self.neural_network.grad_w[layer] = self.neural_network.grad_w[layer] + decay_wt  # Adding weight decay to gradients\n",
        "            self.neural_network.weight[layer] = self.neural_network.weight[layer] - eta * self.neural_network.grad_w[layer]  # Updating weights\n",
        "            self.neural_network.bias[layer] = self.neural_network.bias[layer] - eta * self.neural_network.grad_b[layer]  # Updating biases\n",
        "        \n",
        "    def update(self, t):\n",
        "        \"\"\"\n",
        "        Method to update network parameters based on the selected optimization algorithm.\n",
        "        \"\"\"\n",
        "        if self.optimizer == \"sgd\":\n",
        "            self.stochastic_gradient_decent()\n",
        "        elif self.optimizer == \"mgd\":\n",
        "            self.momentum_based_gradient_decent()\n",
        "        elif self.optimizer == \"nag\":\n",
        "            self.nesterov_accelerated_gradient_decent()\n",
        "        elif self.optimizer == \"rmsprop\":\n",
        "            self.rmsprop()\n",
        "        elif self.optimizer == \"adam\":\n",
        "            self.adam(t)\n",
        "        elif self.optimizer == \"nadam\":\n",
        "            self.nadam(t)\n",
        "             \n",
        "\n",
        "    def momentum_based_gradient_decent(self):\n",
        "        \"\"\"\n",
        "        Method to perform momentum-based gradient descent optimization.\n",
        "        \"\"\"\n",
        "        weight_decay = self.weight_decay\n",
        "        beta = self.beta\n",
        "\n",
        "        for layer in range(len(self.neural_network.size_list)-1, 0, -1):\n",
        "                decay_wt = weight_decay * self.neural_network.weight[layer]  # Applying weight decay\n",
        "                self.neural_network.grad_w[layer] = self.neural_network.grad_w[layer] + decay_wt  # Adding weight decay to gradients\n",
        "\n",
        "                uw = beta * self.neural_network.prv_w[layer] + self.eta * self.neural_network.grad_w[layer]  # Computing update for weights\n",
        "                ub = beta * self.neural_network.prv_b[layer] + self.eta * self.neural_network.grad_b[layer]  # Computing update for biases\n",
        "\n",
        "                self.neural_network.weight[layer] -= uw  # Updating weights\n",
        "                self.neural_network.bias[layer] -= ub  # Updating biases\n",
        "\n",
        "                self.neural_network.prv_w[layer] = uw  # Storing previous weight update\n",
        "                self.neural_network.prv_b[layer] = ub  # Storing previous bias update\n",
        "\n",
        "\n",
        "    def nesterov_accelerated_gradient_decent(self):\n",
        "        \"\"\"\n",
        "        Method to perform Nesterov Accelerated Gradient Descent optimization.\n",
        "        \"\"\"\n",
        "        for layer in range(len(self.neural_network.size_list)-1, 0, -1):\n",
        "            decay_wt = self.weight_decay * self.neural_network.weight[layer]  # Applying weight decay\n",
        "            self.neural_network.grad_w[layer] = self.neural_network.grad_w[layer] + decay_wt  # Adding weight decay to gradients\n",
        "            self.neural_network.prv_w[layer] = self.beta * self.neural_network.prv_w[layer] + self.neural_network.grad_w[layer]\n",
        "            self.neural_network.prv_b[layer] = self.beta * self.neural_network.prv_b[layer] + self.neural_network.grad_b[layer]\n",
        "\n",
        "            self.neural_network.weight[layer] -= ((self.eta) * (self.beta * self.neural_network.prv_w[layer] + self.neural_network.grad_w[layer])) \n",
        "            self.neural_network.bias[layer] -= ((self.eta) * (self.beta * self.neural_network.prv_b[layer] + self.neural_network.grad_b[layer])) \n",
        "\n",
        "    def rmsprop(self):\n",
        "        \"\"\"\n",
        "        Method to perform Root Mean Square Propogation optimization.\n",
        "        \"\"\"\n",
        "        for layer in range(len(self.neural_network.size_list) - 1, 0, -1):\n",
        "            decay_wt = self.weight_decay * self.neural_network.weight[layer]  # Applying weight decay\n",
        "            self.neural_network.grad_w[layer] = self.neural_network.grad_w[layer] + decay_wt  # Adding weight decay to gradients\n",
        "\n",
        "            self.neural_network.prv_w[layer] = self.beta * self.neural_network.prv_w[layer] + (1 - self.beta) * (self.neural_network.grad_w[layer] ** 2)\n",
        "            self.neural_network.prv_b[layer] = self.beta * self.neural_network.prv_b[layer] + (1 - self.beta) * (self.neural_network.grad_b[layer] ** 2)\n",
        "\n",
        "            self.neural_network.weight[layer] -= (self.eta / (np.sqrt(self.neural_network.prv_w[layer] + self.epsilon))) * self.neural_network.grad_w[layer]\n",
        "            self.neural_network.bias[layer] -= (self.eta / (np.sqrt(self.neural_network.prv_b[layer] + self.epsilon))) * self.neural_network.grad_b[layer]\n",
        "\n",
        "    \n",
        "    def adam(self, t):\n",
        "        \"\"\"\n",
        "        Method to perform adam optimizer.\n",
        "\n",
        "        Parameter : t (denotes the time stamp in network)\n",
        "        \"\"\"\n",
        "        for layer in range(len(self.neural_network.size_list)-1, 0, -1):\n",
        "            decay_wt = self.weight_decay * self.neural_network.weight[layer]  # Applying weight decay\n",
        "            self.neural_network.grad_w[layer] = self.neural_network.grad_w[layer] + decay_wt  # Adding weight decay to gradients\n",
        "\n",
        "            self.neural_network.prv_w[layer] = self.beta * self.neural_network.prv_w[layer] + (1 - self.beta) * self.neural_network.grad_w[layer]\n",
        "            self.neural_network.prv_b[layer] = self.beta * self.neural_network.prv_b[layer] + (1 - self.beta) * self.neural_network.grad_b[layer]\n",
        "\n",
        "            \n",
        "            self.neural_network.prv2_w[layer] = self.beta2 * self.neural_network.prv2_w[layer] + (1 - self.beta2) * (self.neural_network.grad_w[layer] ** 2)\n",
        "            self.neural_network.prv2_b[layer] = self.beta2 * self.neural_network.prv2_b[layer] + (1 - self.beta2) * (self.neural_network.grad_b[layer] ** 2)\n",
        "\n",
        "            m_w_hat = self.neural_network.prv_w[layer]/(1 - self.beta**t)\n",
        "            m_b_hat = self.neural_network.prv_b[layer]/(1 - self.beta**t)\n",
        "\n",
        "            v_w_hat = self.neural_network.prv2_w[layer]/(1 - self.beta2 ** t)\n",
        "            v_b_hat = self.neural_network.prv2_b[layer]/(1 - self.beta2 ** t)\n",
        "\n",
        "            self.neural_network.weight[layer] -= (self.eta/(np.sqrt(v_w_hat) + self.epsilon)) * m_w_hat\n",
        "            self.neural_network.bias[layer] -= (self.eta/(np.sqrt(v_b_hat) + self.epsilon)) * m_b_hat\n",
        "\n",
        "\n",
        "    def nadam(self, t):\n",
        "        \"\"\"\n",
        "        Method to perform nadam optimizer.\n",
        "\n",
        "        Parameter : t (denotes the time stamp in network)\n",
        "        \"\"\"\n",
        "        for layer in range(len(self.neural_network.size_list)-1, 0, -1):\n",
        "            decay_wt = self.weight_decay * self.neural_network.weight[layer]  # Applying weight decay\n",
        "            self.neural_network.grad_w[layer] = self.neural_network.grad_w[layer] + decay_wt  # Adding weight decay to gradients\n",
        "            \n",
        "            self.neural_network.prv_w[layer] = self.beta * self.neural_network.prv_w[layer] + (1 - self.beta) * self.neural_network.grad_w[layer]\n",
        "            self.neural_network.prv_b[layer] = self.beta * self.neural_network.prv_b[layer] + (1 - self.beta) * self.neural_network.grad_b[layer]\n",
        "\n",
        "            self.neural_network.prv2_w[layer] = self.beta2 * self.neural_network.prv2_w[layer] + (1 - self.beta2) * (self.neural_network.grad_w[layer] ** 2)\n",
        "            self.neural_network.prv2_b[layer] = self.beta2 * self.neural_network.prv2_b[layer] + (1 - self.beta2) * (self.neural_network.grad_b[layer] ** 2)\n",
        "            \n",
        "            m_w_hat = self.neural_network.prv_w[layer]/(1 - self.beta**t)\n",
        "            m_b_hat = self.neural_network.prv_b[layer]/(1 - self.beta**t)\n",
        "\n",
        "            v_w_hat = self.neural_network.prv2_w[layer]/(1 - self.beta2 ** t)\n",
        "            v_b_hat = self.neural_network.prv2_b[layer]/(1 - self.beta2 ** t)\n",
        "\n",
        "            self.neural_network.weight[layer] -= (self.eta/(np.sqrt(v_w_hat) + self.epsilon)) * (self.beta * m_w_hat + ((1 - self.beta) * self.neural_network.grad_w[layer])/(1 - self.beta ** t))\n",
        "            self.neural_network.bias[layer] -= (self.eta/(np.sqrt(v_b_hat) + self.epsilon)) * (self.beta * m_b_hat + ((1 - self.beta) * self.neural_network.grad_b[layer])/(1 - self.beta ** t))\n",
        "            \n",
        "\n",
        "            \n",
        "\n",
        "    \n",
        "\n",
        "\n",
        "        \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# **Neural Network**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "psiySPMQWE4T"
      },
      "outputs": [],
      "source": [
        "class Neural_Network:\n",
        "    def __init__(self, PARAM) -> None:\n",
        "        \"\"\"\n",
        "        Constructor method for Neural_Network class.\n",
        "        \n",
        "        Parameters:\n",
        "        - PARAM: Dictionary containing network parameters (input size, hidden layer sizes, output size,\n",
        "                 activation function, training input, training output)\n",
        "        \"\"\"\n",
        "        self.weight = {}  # Dictionary to store weights of each layer\n",
        "        self.bias = {}    # Dictionary to store biases of each layer\n",
        "        self.a = {}       # Dictionary to store preactivation of each layer\n",
        "        self.h = {}       # Dictionary to store activations of each layer\n",
        "        self.grad_w = {}  # Dictionary to store gradients of weights for each layer\n",
        "        self.grad_b = {}  # Dictionary to store gradients of biases for each layer\n",
        "        self.prv_w = {}   # Dictionary to store previous weights for momentum-based optimization\n",
        "        self.prv_b = {}   # Dictionary to store previous biases for momentum-based optimization\n",
        "        self.activation_function = PARAM[\"activation_function\"]  # Activation function for hidden layers\n",
        "        self.y_true = PARAM[\"training_output\"]  # True labels for training data\n",
        "        self.input = PARAM[\"training_input\"]     # Input data for training\n",
        "        self.size_list = [PARAM[\"input_size\"]] + PARAM[\"hidden_layers\"] + [PARAM[\"output_size\"]]  # Sizes of all layers\n",
        "        self.act = Activation_Functions()  # Instance of Activation_Functions class\n",
        "        self.derivative = Derivatives()     # Instance of Derivatives class\n",
        "        self.prv2_w = {}\n",
        "        self.prv2_b = {}\n",
        "\n",
        "    def initialize_parameters(self):\n",
        "        \"\"\"\n",
        "        Method to initialize weights and biases of the network.\n",
        "        \"\"\"\n",
        "        for layer in range(1, len(self.size_list)):\n",
        "            self.weight[layer] = np.random.randn(self.size_list[layer-1], self.size_list[layer])  # Initializing weights with random values\n",
        "            # self.weight[layer] = np.zeros((self.size_list[layer-1], self.size_list[layer]))  # Initializing weights with random values\n",
        "            self.bias[layer] = np.random.rand(1, self.size_list[layer])  # Initializing biases with random values\n",
        "            # self.bias[layer] = np.zeros((1, self.size_list[layer]))  # Initializing biases with random values\n",
        "            self.prv_w[layer] = np.zeros((self.size_list[layer-1], self.size_list[layer]))  # Initializing previous weights for momentum-based optimization with zero\n",
        "            self.prv_b[layer] = np.zeros((1, self.size_list[layer]))  # Initializing previous biases for momentum-based optimization with zero\n",
        "            self.prv2_w[layer] = np.zeros((self.size_list[layer-1], self.size_list[layer]))  # Initializing previous weights for momentum-based optimization with zero\n",
        "            self.prv2_b[layer] = np.zeros((1, self.size_list[layer]))  # Initializing previous biases for momentum-based optimization with zero\n",
        "\n",
        "    def forward_propagation(self, x):\n",
        "        \"\"\"\n",
        "        Method to perform forward propagation through the network.\n",
        "        \n",
        "        Parameters:\n",
        "        - x: Input data\n",
        "        \n",
        "        Returns:\n",
        "        - Output of the final layer (after applying softmax activation)\n",
        "        \"\"\"\n",
        "        self.h[0] = x  # Input layer\n",
        "        for layer in range(1, len(self.size_list)-1):\n",
        "            self.a[layer] = np.dot(self.h[layer-1], self.weight[layer]) + self.bias[layer]  # Computing weighted sum of inputs\n",
        "            self.h[layer] = self.act.activation(self.a[layer], self.activation_function)  # Applying activation function\n",
        "        self.a[layer+1] = np.dot(self.h[layer], self.weight[layer+1]) + self.bias[layer+1]  # Computing weighted sum for final layer\n",
        "        self.h[layer+1] = self.act.activation(self.a[layer+1], \"softmax\")  # Applying softmax activation\n",
        "        return self.h[layer+1]  # Returning output of final layer\n",
        "\n",
        "    def backward_propagation(self, input, y_true, y_hat):\n",
        "        \"\"\"\n",
        "        Method to perform backward propagation through the network.\n",
        "        \n",
        "        Parameters:\n",
        "        - input: Input data\n",
        "        - y_true: True labels\n",
        "        - y_hat: Predicted probabilities\n",
        "        \n",
        "        Returns:\n",
        "        - Gradients of weights and biases\n",
        "        \"\"\"\n",
        "        error_wrt_output = -(y_true - y_hat)  # Computing error with respect to output layer\n",
        "\n",
        "        for layer in range(len(self.size_list)-1, 1, -1):\n",
        "            self.grad_w[layer] = np.dot(self.h[layer-1].T, error_wrt_output)  # Computing gradients of weights\n",
        "            self.grad_b[layer] = np.sum(error_wrt_output, axis=0, keepdims=True)  # Computing gradients of biases\n",
        "\n",
        "            error_wrt_hidden = np.dot(error_wrt_output, self.weight[layer].T)  # Computing error with respect to hidden layer\n",
        "            error_wrt_output = error_wrt_hidden * self.derivative.derivatives(self.a[layer-1], self.activation_function)  # Computing error with respect to output of hidden layer\n",
        "        self.grad_w[1] = np.dot(input.T, error_wrt_output)  # Computing gradients of weights for input layer\n",
        "        self.grad_b[1] = np.sum(error_wrt_output, axis=0, keepdims=True)  # Computing gradients of biases for input layer\n",
        "        return self.grad_w, self.grad_b  # Returning gradients of weights and biases\n",
        "\n",
        "    def one_hot_vector(self, y_true):\n",
        "        \"\"\"\n",
        "        Method to convert true labels into one-hot vectors.\n",
        "        \n",
        "        Parameters:\n",
        "        - y_true: True labels\n",
        "        \n",
        "        Returns:\n",
        "        - One-hot vector representation of true labels\n",
        "        \"\"\"\n",
        "        vec = np.zeros(10)  # Initializing one-hot vector\n",
        "        vec[y_true] = 1  # Setting the corresponding index to 1\n",
        "        return vec\n",
        "\n",
        "    def one_hot_matrix(self, y_true):\n",
        "        \"\"\"\n",
        "        Method to convert true labels into one-hot matrices.\n",
        "        \n",
        "        Parameters:\n",
        "        - y_true: True labels\n",
        "        \n",
        "        Returns:\n",
        "        - One-hot matrix representation of true labels\n",
        "        \"\"\"\n",
        "        row = y_true.shape[0]  # Number of samples\n",
        "        col = 10  # Number of classes\n",
        "        mat = np.zeros((row, col))  # Initializing one-hot matrix\n",
        "        for i in range(row):\n",
        "            mat[i][y_true[i]] = 1  # Setting the corresponding index to 1\n",
        "        return mat\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Train Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [],
      "source": [
        "class Train_Model:\n",
        "    def __init__(self, neural_network, optimizer) -> None:\n",
        "        \"\"\"\n",
        "        Constructor method for Train_Model class.\n",
        "        \n",
        "        Parameters:\n",
        "        - PARAM_NEURAL_NETWORK: Dictionary containing parameters for the neural network\n",
        "        - PARAM_OPTIMIZER: Dictionary containing parameters for the optimizer\n",
        "        \"\"\"\n",
        "        self.neural_network =neural_network  # Neural network instance\n",
        "        self.optimizer = optimizer  # Optimizer instance\n",
        "        self.loss = Loss_Function()  # Loss function instance\n",
        "\n",
        "    def compute_performance(self, data, label):\n",
        "        y_predictions = self.neural_network.forward_propagation(data)\n",
        "        labels = self.neural_network.one_hot_matrix(label)\n",
        "        accuracy = np.sum(np.argmax(y_predictions, axis=1) == np.argmax(labels, axis = 1))\n",
        "        loss = self.loss.compute_loss(labels, y_predictions, \"cross_entropy\")\n",
        "        return loss, (accuracy/len(data)) * 100\n",
        "                \n",
        "    def fit_data(self, batch_size, epochs):\n",
        "        \"\"\"\n",
        "        Method to train the model on the given dataset.\n",
        "        \n",
        "        Parameters:\n",
        "        - batch_size: Size of each batch\n",
        "        - epochs: Number of epochs for training\n",
        "        \"\"\"\n",
        "        x_test_ = input_matrix(x_test)\n",
        "        self.neural_network.initialize_parameters()  # Initializing parameters of the neural network\n",
        "        total_batches = int(np.ceil(self.neural_network.input.shape[0] / batch_size))  # Total number of batches\n",
        "        for i in range(epochs):\n",
        "            loss = 0  # Initializing loss for each epoch\n",
        "            accuracy = 0  # Initializing accuracy for each epoch\n",
        "            t = 1\n",
        "            for batch in range(total_batches):\n",
        "                batch_start = batch * batch_size  # Starting index of the batch\n",
        "                batch_end = batch_start + batch_size  # Ending index of the batch\n",
        "                image_set = self.neural_network.input[batch_start : batch_end]  # Extracting batch of input images\n",
        "                res_set = self.neural_network.y_true[batch_start : batch_end]  # Extracting batch of true labels\n",
        "                \n",
        "                # if self.optimizer.optimizer == \"nag\":\n",
        "                #     for layer in range(1, len(self.neural_network.size_list)):\n",
        "                #         self.neural_network.weight[layer] -= (self.optimizer.beta * self.neural_network.prv_w[layer])\n",
        "                #         self.neural_network.bias[layer] -= (self.optimizer.beta * self.neural_network.prv_b[layer])\n",
        "                    \n",
        "                y_hat = self.neural_network.forward_propagation(image_set)  # Forward propagation\n",
        "                res = self.neural_network.one_hot_matrix(res_set)  # Converting true labels into one-hot matrices\n",
        "                grad_w , grad_b = self.neural_network.backward_propagation(image_set, res, y_hat)  # Backward propagation\n",
        "                    \n",
        "                for layer in range(1, len(self.neural_network.size_list)):\n",
        "                    self.neural_network.grad_w[layer] = grad_w[layer]/batch_size  # Normalizing gradients of weights\n",
        "                    self.neural_network.grad_b[layer] = grad_b[layer]/batch_size  # Normalizing gradients of biases\n",
        "\n",
        "                self.optimizer.update(t)  # Updating weights and biases using optimizer\n",
        "                t += 1\n",
        "                # for img in range(y_hat.shape[0]):\n",
        "                #     if np.argmax(y_hat[img]) == np.argmax(res[img]):  # Calculating accuracy\n",
        "                #         accuracy += 1\n",
        "                # loss += self.loss.compute_loss(res, y_hat, \"cross_entropy\")  # Calculating loss\n",
        "            \n",
        "            t_loss, t_acc = self.compute_performance(self.neural_network.input, self.neural_network.y_true)\n",
        "            v_loss, v_acc = self.compute_performance(x_test_, y_test)\n",
        "\n",
        "            print(f\"epoch:{i+1} :: \\n Training-loss : {t_loss}, Training-accuracy:{t_acc}%\")    # Printing loss and accuracy for each epoch\n",
        "            print(f\"Validation-loss : {v_loss}, Validation-accuracy:{v_acc}%\\n\\n\")    # Printing loss and accuracy for each epoch\n",
        "\n",
        "\n",
        "         "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# **Training**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vP1vqvWmWE4U",
        "outputId": "21011f6d-795c-4f12-f2c7-957e1090e3ce"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch:1 :: \n",
            " Training-loss : 0.5228505381229007, Training-accuracy:81.56333333333333%\n",
            "Validation-loss : 0.5622777022628573, Validation-accuracy:80.31%\n",
            "\n",
            "\n",
            "epoch:2 :: \n",
            " Training-loss : 0.47928071911678316, Training-accuracy:82.98833333333333%\n",
            "Validation-loss : 0.5359963431376357, Validation-accuracy:81.51%\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "\n",
        "training_input = input_matrix(x_train)\n",
        "\n",
        "PARAM_NEURAL_NETWORK = {\n",
        "    \"input_size\": training_input.shape[1],\n",
        "    \"output_size\": len(label),\n",
        "    \"hidden_layers\": [32, 64, 128],\n",
        "    \"activation_function\": \"sigmoid\",\n",
        "    \"training_output\": y_train,\n",
        "    \"training_input\": training_input,\n",
        "}\n",
        "\n",
        "PARAM_OPTIMIZER = {\n",
        "    \"eta\": 0.005,\n",
        "    \"optimizer\": \"nadam\",\n",
        "    \"beta\": 0.9,\n",
        "    \"weight_decay\": 0.00001,\n",
        "    \"epsilon\": 1e-8,\n",
        "    \"beta2\" : 0.999\n",
        "}\n",
        "\n",
        "neural_network = Neural_Network(PARAM_NEURAL_NETWORK)\n",
        "optimizer = Optimizer(neural_network, PARAM_OPTIMIZER)\n",
        "\n",
        "my_model = Train_Model(neural_network, optimizer)\n",
        "my_model.fit_data(batch_size=16, epochs=5)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "t6cLnVpbWE4U"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.3887\n",
            "[1 2 3 5 7 8 9]\n"
          ]
        }
      ],
      "source": [
        "test_data = input_matrix(x_test)\n",
        "y_pred1 = my_model.neural_network.forward_propagation(test_data)\n",
        "# y_pred2 = n2.forward_propogation(test_data, \"tanh\")\n",
        "accuracy1 = np.mean(np.argmax(y_pred1, axis = 1) == y_test)\n",
        "# accuracy2 = np.mean(np.argmax(y_pred2, axis = 1) == y_train)\n",
        "\n",
        "print(accuracy1)\n",
        "print(np.unique(np.argmax(y_pred1, axis = 1)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dw7cXdfbWE4V"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xw8MT0ipWE4V"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vGxU9KkzWE4W"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pZr5vni3WE4W",
        "outputId": "3080ee62-0cac-45c2-88a7-2c27d90473ef"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "30XFtd4GWE4a"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
